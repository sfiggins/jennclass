{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 4"
      ],
      "metadata": {
        "id": "6uTQ785jdpHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1"
      ],
      "metadata": {
        "id": "3L8BIUQtdsau"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gy6uYrOVdjta",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "outputId": "c3e167d9-2676-4f49-c645-1f645af7be95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the dtm1 shape:  (1145, 23937)\n",
            "['the', 'of', 'to', 'na', 'and']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anywh', 'becau', 'el', 'elsewh', 'everywh', 'ind', 'otherwi', 'plea', 'somewh'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-32e4ac8b3c42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0mtrain_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_inds'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mtest_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_inds'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mtrain_dtm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtm1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_inds'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_inds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0mtest_dtm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtm1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_inds'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# Dispatch to specialized methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m_validate_indices\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0mrow\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_asindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misintlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m_asindices\u001b[0;34m(self, idx, length)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Index dimension must be 1 or 2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Index dimension must be 1 or 2"
          ]
        }
      ],
      "source": [
        "#1.0 preprocessing\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from nltk.stem import PorterStemmer\n",
        "cv = CountVectorizer()\n",
        "ps = PorterStemmer()\n",
        "tv = TfidfVectorizer()\n",
        "from string import punctuation, digits\n",
        "import nltk\n",
        "import re\n",
        "import pickle\n",
        "import numpy\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score\n",
        "\n",
        "words = []\n",
        "docs1 = []\n",
        "\n",
        "# load partition\n",
        "with open(\"partition_hw4.pkl\", \"rb\") as f:\n",
        "  partition = pickle.load(f)\n",
        "\n",
        "# make preprocessing function\n",
        "def preprocess_text(text):\n",
        "  text = text.lower() #make everything lower case\n",
        "  text = text.translate(str.maketrans('','', digits)) # take out digits\n",
        "  text = text.translate(str.maketrans('','', punctuation)) # take out punctuation\n",
        "  text = re.sub(' +', ' ', text).strip() # remove extra spaces\n",
        "  t_list = text.split(\" \")\n",
        "  text = \" \".join(ps.stem(word) for word in t_list) #stemming\n",
        "  return text\n",
        "\n",
        "mystopwords = list(ENGLISH_STOP_WORDS)\n",
        "mystopwords.sort()\n",
        "stem_stopwords = [ps.stem(word) for word in mystopwords]\n",
        "\n",
        "# put file into dataframe\n",
        "df = pd.read_csv(\"news_label.csv\", encoding='latin1')\n",
        "docs1 = df['Message'].tolist() # make a list\n",
        "labels1 = df[\"Class\"]\n",
        "\n",
        "#1.1 myvocab1: unigrams\n",
        "cv1 = CountVectorizer(preprocessor = preprocess_text,\n",
        "                     stop_words = stem_stopwords,\n",
        "                     min_df=2,\n",
        "                     ngram_range = (1,1))\n",
        "dtm1 = cv.fit_transform(df['Message'])\n",
        "myvocab = cv.get_feature_names_out()\n",
        "myvocab_counts = dtm1.sum(axis=0).tolist()[0]\n",
        "myvocab1 = [myvocab[i] for i in range(len(myvocab)) if myvocab_counts[i] >= 100]\n",
        "print(\"Here is the dtm1 shape: \", dtm1.shape)\n",
        "terms = cv.get_feature_names_out()\n",
        "freq_num = dtm1.sum(axis=0)[0].tolist()[0]\n",
        "ctf = pd.DataFrame({\"terms\": terms, \n",
        "                    \"freq\": freq_num})\n",
        "ctf.sort_values(\"freq\", ascending=False, inplace=True)\n",
        "print(ctf.iloc[:5][\"terms\"].tolist())\n",
        "\n",
        "cv12 = CountVectorizer(preprocessor = preprocess_text,\n",
        "                     stop_words = stem_stopwords,\n",
        "                     vocabulary=myvocab1)\n",
        "dmt12 = cv12.fit_transform(df['Message'])\n",
        "\n",
        "\n",
        "\n",
        "# partition data\n",
        "train_inds = partition['train_inds']\n",
        "test_inds = partition['test_inds']\n",
        "train_dtm1 = dtm1['train_inds']\n",
        "train_labels = df.loc['train_inds', 'Class']\n",
        "test_dtm1 = dtm1['test_inds']\n",
        "test_labels = df.loc['test_inds', 'Class']\n",
        "\n",
        "# train for KNN classifier\n",
        "k = 7\n",
        "knn = KNeighborsClassifier(n_neighbors=k)\n",
        "knn.fit(train_dtm1, train_labels)\n",
        "\n",
        "# test the KNN case\n",
        "pred_label_knn1 = knn.predict(test_dtml1)\n",
        "accuracy_knn1 = knn.score(test_dtm1, test_labels)\n",
        "precision_knn1 = precision_score(test_labels, pred_label_knn1, pos_label = 'space')\n",
        "recall_knn1 = recall_score(test_labels, pre_label_knn1, pos_label = 'space')\n",
        "\n",
        "#1.3 pred_label_knn1:\n",
        "print(\"Printing pred_label_knn1:\")\n",
        "print(pred_label_knn1)\n",
        "\n",
        "#1.4 accuracy_knn1:\n",
        "print(\"Printing accuracy_knn1:\")\n",
        "print(accuracy_knn1)\n",
        "\n",
        "#1.5 precision_knn1:\n",
        "print(\"Printing precision_knn1:\")\n",
        "print(precision_knn1)\n",
        "\n",
        "#1.6 recall_knn1:\n",
        "print(\"Printing recall_knn1:\")\n",
        "print(recall_knn1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2"
      ],
      "metadata": {
        "id": "0lvxu1uSeGP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Continuation of problem 1:\n",
        "\n",
        "# Work to impore the kNN model, to get accuacy on the testing set to above 90%"
      ],
      "metadata": {
        "id": "prn3FOHzeIBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 3"
      ],
      "metadata": {
        "id": "AZbyqaxueWy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Continuation of problem 2:\n",
        "\n",
        "# Build neural network models\n",
        "\n",
        "# 3.1 h_val_auc:\n",
        "\n",
        "# 3.2 best_model:\n"
      ],
      "metadata": {
        "id": "QRSXPf-TeYLv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}