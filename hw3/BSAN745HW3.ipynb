{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCAb9TR7-gOY"
      },
      "source": [
        "Assignment 3 - BSAN745 - Ludlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUimk9d0_ByL"
      },
      "source": [
        "Problem 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8SEg7IC5-c16",
        "outputId": "982e4d80-5bf2-4a58-edc0-64ba3140c632"
      },
      "outputs": [],
      "source": [
        "#1.1 docs1: make a list of Sully reviews\n",
        "\n",
        "import pandas as pd # for reading dataframes\n",
        "import regex as re\n",
        "import numpy as np\n",
        "import sklearn as sk\n",
        "\n",
        "# get sully.csv into df, then into a list of reviews\n",
        "\n",
        "docs1 = [] # future list\n",
        "\n",
        "# put file into dataframe\n",
        "df = pd.read_csv(\"sully.csv\")\n",
        "# print(\"Printing list of Sully reviews, docs1:\")\n",
        "docs1 = df['review'].tolist() # make a list\n",
        "# print(docs1)\n",
        "\n",
        "\n",
        "# 1.2 mystopwords: make a vector of default stopwords but not \"not\" and \"*n't\"\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from string import punctuation, digits\n",
        "import re\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer()\n",
        "\n",
        "ps = PorterStemmer() # stemming\n",
        "                          # documentations says it makes words all lowercase\n",
        "                          # in addition to reducing words to their root words\n",
        "                          # need to ck\n",
        "\n",
        "# Pre-Processing requirements:\n",
        "#1.2.1. turn all letters to lowercase <- done\n",
        "#1.2.2. remove punctuation and numbers <- done\n",
        "#1.2.3. remove default stop words excludingg \"not\" & \"*n't\" # skipping not needed\n",
        "#1.2.4. remove extra blank spaces <- done\n",
        "#1.2.5. apply stemming - ps <- done\n",
        "#1.2.6. generate unigrams <- done\n",
        "\n",
        "# make preprocessing function\n",
        "def preprocess_text(text):\n",
        "  text = text.lower() # make everything lower case\n",
        "  text = text.translate(str.maketrans('','', digits)) # take out digits\n",
        "  text = text.translate(str.maketrans('','', punctuation)) # take out punctuation\n",
        "  text = re.sub(' +', ' ', text).strip() # remove extra spaces\n",
        "  t_list = text.split(\" \")\n",
        "  text = \" \".join(ps.stem(word) for word in t_list) #stemming & tokenizing\n",
        "  return text\n",
        "\n",
        "mystopwords = list(ENGLISH_STOP_WORDS)\n",
        "mystopwords.remove(\"not\")\n",
        "mystopwords = [word for word in mystopwords if not word.endswith(\"n't\")]\n",
        "\n",
        "# stem stopwords\n",
        "mystopwords = [ps.stem(word) for word in mystopwords]\n",
        "\n",
        "# print(\"Printing mystopwords:\")\n",
        "# print(mystopwords)\n",
        "\n",
        "# 1.3 dtm1: filter and make a dtm using preprocessing criteria\n",
        "cv = CountVectorizer(preprocessor=preprocess_text, stop_words = mystopwords,\n",
        "                     ngram_range=(1,1))\n",
        "\n",
        "dtm1 = cv.fit_transform(docs1)\n",
        "\n",
        "# print(\"Printing dtm1:\")\n",
        "# print(dtm1)\n",
        "\n",
        "# 1.4 tf_dict1: make a python dictionary with terms as keys and freq of terms as values\n",
        "tf_dict1 = {}\n",
        "terms = cv.get_feature_names_out()\n",
        "freq_sum = dtm1.sum(axis=0)[0].tolist()[0] # to make a list to select first element\n",
        "dict1 = pd.DataFrame({\"terms\":terms, \"freq\":freq_sum})\n",
        "dict1 = dict1[dict1['freq']<=50]\n",
        "dict1.sort_values(\"freq\",ascending=False,inplace=True)\n",
        "\n",
        "# print(\"Printing dict1:\")\n",
        "# print(dict1)\n",
        "\n",
        "# 1.5 worldcld: make a WordCloud with specific call, and other listed specs\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "terms = cv.get_feature_names_out()\n",
        "freq_sum = dtm1.sum(axis=0)[0].tolist()[0]\n",
        "dict1 = dict(zip(terms, freq_sum))\n",
        "worldcld = WordCloud(background_color = \"white\", max_words = 20, random_state = 9)\n",
        "worldcld.generate_from_frequencies(dict1)\n",
        "# show\n",
        "print(\"Printing a WordCloud:\")\n",
        "plt.imshow(worldcld, interpolation = \"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reh9I0fp_D1H"
      },
      "source": [
        "Problem 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W-U4zs-q_E-J",
        "outputId": "54907a70-60fb-497c-e2be-1920b9fada27"
      },
      "outputs": [],
      "source": [
        "# same thing but with bigrams\n",
        "\n",
        "#2.0 make a list of Sully reviews\n",
        "\n",
        "import pandas as pd # for reading dataframes\n",
        "import regex as re\n",
        "import numpy as np\n",
        "import sklearn as sk\n",
        "\n",
        "# get sully.csv into df, then into a list of reviews\n",
        "\n",
        "docs2 = [] # future list\n",
        "\n",
        "# put file into dataframe\n",
        "df2 = pd.read_csv(\"sully.csv\")\n",
        "# print(\"Printing list of Sully reviews, docs1:\")\n",
        "docs2 = df2['review'].tolist() # make a list\n",
        "# print(docs2)\n",
        "\n",
        "\n",
        "# 2.0.1 mystopwords: make a vector of default stopwords but not \"not\" and \"*n't\"\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from string import punctuation, digits\n",
        "import re\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "ps = PorterStemmer() # stemming\n",
        "                          # documentations says it makes words all lowercase\n",
        "                          # in addition to reducing words to their root words\n",
        "                          # need to ck\n",
        "\n",
        "# Pre-Processing requirements:\n",
        "# turn all letters to lowercase\n",
        "# remove punctuation and numbers\n",
        "# remove default stop words excludingg \"not\" & \"*n't\" # skipping not needed\n",
        "# remove extra blank spaces\n",
        "# apply stemming - ps\n",
        "# generate unigrams\n",
        "\n",
        "# make preprocessing function\n",
        "def preprocess_text(text):\n",
        "  text = text.lower() # make everything lower case\n",
        "  text = text.translate(str.maketrans('','', digits)) # take out digits\n",
        "  text = text.translate(str.maketrans('','', punctuation)) # take out punctuation\n",
        "  text = re.sub(' +', ' ', text).strip() # remove extra spaces\n",
        "  t_list = text.split(\" \")\n",
        "  text = \" \".join(ps.stem(word) for word in t_list) #stemming & tokenizing\n",
        "  return text\n",
        "\n",
        "mystopwords = list(ENGLISH_STOP_WORDS)\n",
        "mystopwords.remove(\"not\")\n",
        "mystopwords = [word for word in mystopwords if not word.endswith(\"n't\")]\n",
        "\n",
        "# stem stopwords\n",
        "mystopwords = [ps.stem(word) for word in mystopwords]\n",
        "#print(\"Printing mystopwords:\")\n",
        "#print(mystopwords)\n",
        "\n",
        "\n",
        "# 2.1 dtm2: filter and make a dtm using preprocessing criteria\n",
        "cv = CountVectorizer(preprocessor=preprocess_text, stop_words = mystopwords,\n",
        "                     ngram_range=(2,2))\n",
        "dtm2 = cv.fit_transform(docs2)\n",
        "# print(\"Printing dtm2:\")\n",
        "# print(dtm2)\n",
        "\n",
        "# 2.2 Ctf:\n",
        "terms2 = cv.get_feature_names_out()\n",
        "freq_sum2 = dtm2.sum(axis=0)[0].tolist()[0] # list within a list to get 1st element\n",
        "Ctf2 = pd.DataFrame({\"terms\": terms2,\n",
        "                              \"freq\": freq_sum2})\n",
        "Ctf2.sort_values(\"freq\",ascending=False,inplace=True)\n",
        "# print(\"Printing Ctf2:\")\n",
        "# print(Ctf2)\n",
        "\n",
        "# 2.3 terms2: and freq2:\n",
        "\n",
        "terms2 = Ctf2[Ctf2['freq']>=20]\n",
        "terms2.sort_values(\"freq\", ascending=False,inplace=True)\n",
        "# print(\"Printing top 20 - terms2:\")\n",
        "# print(terms2)\n",
        "freq2 = Ctf2.nlargest(20,\"freq\")\n",
        "# print(\"Printing top 20 - freq2:\")\n",
        "# print(freq2)\n",
        "\n",
        "# 2.4 Histogram plot\n",
        "bigrams = Ctf2[Ctf2[\"freq\"]>20][\"terms\"].tolist()\n",
        "# print(\"Printing bigram list:\")\n",
        "# print(bigrams)\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "Ctf2.sort_values(\"freq\",ascending=False,inplace=True)\n",
        "ctf_terms = Ctf2.iloc[:20][\"terms\"].tolist()\n",
        "ctf_vals = Ctf2.iloc[:20][\"freq\"].tolist()\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize = (10,5))\n",
        "plt.bar(ctf_terms, ctf_vals, color = 'green',\n",
        "        width = 0.4)\n",
        "plt.xticks(rotation = 90)\n",
        "plt.xlabel(\"Terms\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Bigram Term Frequency\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HEopsO3_Fd6"
      },
      "source": [
        "Problem 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxgNn2Z9_Grn",
        "outputId": "c2b22aa8-ab31-476a-a8fe-0430ded8be12"
      },
      "outputs": [],
      "source": [
        "#3.0 prep \n",
        "\n",
        "import pandas as pd # for reading dataframes\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "#!pip install NRCLex\n",
        "nltk.download('punkt')\n",
        "from nltk import tokenize\n",
        "\n",
        "# 3.1 sentiment_score: list of compound sentiments\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "Complaints_df = pd.read_csv(\"Complaints.csv\", encoding=\"latin1\")\n",
        "\n",
        "sentiment_score = []\n",
        "\n",
        "for complaint in Complaints_df['Consumer.complaint.narrative']:\n",
        "  score = sia.polarity_scores(complaint)['compound']\n",
        "  sentiment_score.append(score)\n",
        "\n",
        "# print(\"Printing sentiment_scores:\")\n",
        "# print(sentiment_score)\n",
        "\n",
        "\n",
        "# 3.2 doc_score_df:\n",
        "doc_score_df = pd.DataFrame({\"Complaints\":Complaints_df[\"Consumer.complaint.narrative\"], \n",
        "                             \"Sentiment_Score\": sentiment_score})\n",
        "# print(\"Printing doc_score_df:\")\n",
        "# print(doc_score_df)\n",
        "\n",
        "\n",
        "# 3.3 Common elements to 5 most negative complaints\n",
        "doc_score_df.sort_values(by = \"Sentiment_Score\", inplace=True)\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "print(\"Printing 5 most negative complaints:\")\n",
        "print(doc_score_df.head(5))\n",
        "print(\"Complaint words include: sickened, fretful, fraud, abusive, crying, horror. Highly negative sentiment.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF3XJ1vS_Ha_"
      },
      "source": [
        "Problem 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWWa0A9G_ImC",
        "outputId": "f3352356-e1bd-41e9-bbab-d7352038f2eb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "df = pd.read_csv('news_label.csv', encoding=\"latin1\")\n",
        "\n",
        "def preprocess(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub(r'[^\\w\\s]','',text)\n",
        "  text = re.sub(r'\\d+','',text)\n",
        "  stopwords_list = stopwords.words('english')\n",
        "  text = ' '.join([word for word in text.split() if word not in stopwords_list])\n",
        "  text = text.strip()\n",
        "  ps = PorterStemmer()\n",
        "  text = ' '.join([ps.stem(word) for word in text.split()])\n",
        "  return text\n",
        "\n",
        "df['Message'] = df['Message'].apply(preprocess)\n",
        "word_freq = {}\n",
        "for text in df['Message']:\n",
        "  for word in text.split():\n",
        "    if word not in word_freq:\n",
        "      word_freq[word] = 0\n",
        "    word_freq[word] += 1\n",
        "custom_dict = {k:v for k, v in word_freq.items() if v >= 100}\n",
        "\n",
        "tv = TfidfVectorizer(vocabulary=custom_dict.keys())\n",
        "dtm = tv.fit_transform(df['Message'])\n",
        "tv.get_feature_names_out()\n",
        "\n",
        "k = 10\n",
        "lsa = TruncatedSVD(n_components=k)\n",
        "lsa_D = lsa.fit_transform(dtm)\n",
        "ttm_df = pd.DataFrame(lsa.components_,columns=tv.get_feature_names_out())\n",
        "\n",
        "# 4.1 term4: For Topic 3:\n",
        "print(\"Printing term4, info for Topic 3:\")\n",
        "topic_index = 2\n",
        "topic = lsa.components_[topic_index]\n",
        "top_index = np.argsort(-topic)[:10]\n",
        "top_terms = [list(custom_dict.keys())[i] for i in top_index]\n",
        "top_scores = [topic[i] for i in top_index]\n",
        "term4 = pd.DataFrame({'Term': top_terms, 'Score': top_scores})\n",
        "print(term4)\n",
        "\n",
        "# 4.2 term5: For Topic 5:\n",
        "print(\"Printing info for Topic 5:\")\n",
        "topic_index = 4\n",
        "topic = lsa.components_[topic_index]\n",
        "top_idx = np.argsort(-topic)[:10]\n",
        "top_terms = [list(custom_dict.keys())[i] for i in top_idx]\n",
        "top_scores = [topic[i] for i in top_idx]\n",
        "term5 = pd.DataFrame({'Term': top_terms, 'Score': top_scores})\n",
        "print(term5)\n",
        "\n",
        "# For Topic 4:\n",
        "print(\"Printing info for Topic 4:\")\n",
        "topic_index = 3\n",
        "topic = lsa.components_[topic_index]\n",
        "top_idx = np.argsort(-topic)[:10]\n",
        "top_terms = [list(custom_dict.keys())[i] for i in top_idx]\n",
        "top_scores = [topic[i] for i in top_idx]\n",
        "term6 = pd.DataFrame({'Term': top_terms, 'Score': top_scores})\n",
        "print(term6)\n",
        "\n",
        "# Since we have topics med and space, to compare 4 and 5, we can look at scores\n",
        "# of each and compare\n",
        "print(\"#5 more med and #4 is more space.\")\n",
        "\n",
        "# 4.4 I am unable to determine which document is most associated with topic 4.\n",
        "# I don't know how to do this part, I assume I would start with k = number of\n",
        "# documents. I don't know where to go from there.\n",
        "print(\"For the document most associated with topic 4, I am unable to determine this. I assume I would first start with k = count of Messages, but I don't know where to go from there, at this time.\")\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
